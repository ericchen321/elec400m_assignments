{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7b7a29b375721396ebbea159b3b210aa",
     "grade": false,
     "grade_id": "Intro",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Due date is Wednesday, February 17, 2021, 23:59 PST**\n",
    "\n",
    "**ONLY write into the existing cells, and do NOT add any cells.**\n",
    "\n",
    "# ELEC 400M / EECE 571M Assignment 1: Linear models for classification\n",
    "(This assignment is a modified version of an assignment used in ECE 421 at the University of Toronto and kindly made available to us by the instructor.)\n",
    "\n",
    "In this assignment, you will be using linear models discussed in the lectures to perform a binary classification task. You will compare the performances of linear classification and logistic regression using suitable training algorithms. The implementation will be done in python using functions from the **NumPy** library. Please use **NumPy arrays** as data structures.\n",
    "\n",
    "## Data Set\n",
    "We consider the dataset of images of letters contained in file notMNIST.npz. In particular, you will use a smaller dataset that only contains the images from two letter classes: “C” (the positive class) and “J” (the negative class). The images are of size 28 × 28 pixels. The figure below shows 20 randomly selected image samples for the letters “C” and “J”.\n",
    "\n",
    "<img src=\"sample_images.png\" width=\"400\">\n",
    "\n",
    "You will apply the function `loadData` to generate the subset of images containing only letters “C” and “J”. This script organizes the total set of 3,745 images into smaller subsets containing 3,500 training images, 100 validation images and 145 test images. Their use will be further specified in the problem descriptions below.\n",
    "\n",
    "Execute the two helper scripts provided below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "93399949d8e1d624b7e3d4664d94e73e",
     "grade": false,
     "grade_id": "get_started",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    }
   ],
   "source": [
    "%reset\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e510dbfd96e59225033ce7813d87f4b0",
     "grade": false,
     "grade_id": "loadData_function",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def loadData():\n",
    "    with np.load('notMNIST.npz') as data:\n",
    "        Data, Target = data['images'], data['labels']\n",
    "        posClass = 2\n",
    "        negClass = 9\n",
    "        dataIndx = (Target==posClass) + (Target==negClass)\n",
    "        Data = Data[dataIndx]/255.0\n",
    "        Target = Target[dataIndx].reshape(-1, 1)\n",
    "        Target[Target==posClass] = 1\n",
    "        Target[Target==negClass] = 0\n",
    "        np.random.seed(1)\n",
    "        randIndx = np.arange(len(Data))\n",
    "        np.random.shuffle(randIndx)\n",
    "        Data, Target = Data[randIndx], Target[randIndx]\n",
    "        trainData, trainTarget = Data[:3500], Target[:3500]\n",
    "        validData, validTarget = Data[3500:3600], Target[3500:3600]\n",
    "        testData, testTarget = Data[3600:], Target[3600:]\n",
    "           \n",
    "    return trainData, validData, testData, trainTarget, validTarget, testTarget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d173fe8bab6521a53fbc2ac2e376a7cb",
     "grade": false,
     "grade_id": "Linear_Classification",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Linear Classification\n",
    "\n",
    "The first classifier is the linear classifier \n",
    "$$\\hat{y}=\\mathrm{sign}\\left(\\sum_{i=0}^dw_ix_i\\right)\\,$$\n",
    "where $x_0=1$ so that $b=w_0x_0$ is the bias term and $x_1,\\ldots, x_d$ are the input features.\n",
    "The loss function for an input-output pair $(\\underline{x}_n,y_n)$ and given model parameters $\\underline{w}$ is \n",
    "$$L_n(\\underline{w})= \\mathbf{1}\\{\\hat{y}_n\\neq y_n\\}\\;.$$\n",
    "The total loss for $N$ samples is \n",
    "$$L(\\underline{w})= \\frac{1}{N}\\sum\\limits_{n=1}^N\\mathbf{1}\\{\\hat{y}_n\\neq y_n\\}\\;.$$\n",
    "\n",
    "\n",
    "\n",
    "### Notes on classification\n",
    "* The classification should be based on the $d=28\\times 28=784$ intensity values in an image. This means that you need to flatten the 2D images to 1D input vectors $\\underline{x}$ of length 784.\n",
    "* The outputs $\\hat{y}$ of the perceptron model are from $\\{-1,+1\\}$, while the target variable from the data set is from $\\{0,1\\}$. You need to make adjustements to account for this difference, which can include adjusting the data type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1f9913892a5ced7c52d2cd7799daec6d",
     "grade": false,
     "grade_id": "Loss_function",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Loss Function [2 points]\n",
    "\n",
    "Implement a function to compute the classification loss as defined above. The function has three input arguments: the weight vector, the feature vectors, and the labels. The binary labels are assumed to be from $\\{-1,+1\\}$. It returns the total loss (normalized with $N$, as above) associated with the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "94382f8bac4654f2ec503f7a29aff9b7",
     "grade": false,
     "grade_id": "ErrorRate",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def ErrorRate (w, x, y):\n",
    "    \"\"\"Compute error rate for binary linear classifier\"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    # compute y_hat\n",
    "    y_hat = np.matmul(np.transpose(w), np.transpose(x))\n",
    "    y_hat[y_hat<0.0] = -1\n",
    "    y_hat[y_hat>0.0] = 1\n",
    "    \n",
    "    # compute loss\n",
    "    error = np.sum(y_hat != (np.transpose(y))) / y_hat.shape[1]\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "68eaf10b41a88e379a8594a965173870",
     "grade": true,
     "grade_id": "correct-ErrorRate",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Check that ErrorRate returns the correct output for several inputs\"\"\"\n",
    "\"\"\"(You may have to adjust the array format for inputs and output that your function expects and delivers, respectively.) )\"\"\"\n",
    "wtest1 = np.array([[1],[1],[1]])\n",
    "xtest1 = np.array([[1,2,3],[2,3,4],[4,5,6],[-1,-2,-3]])\n",
    "ytest1 = np.array([[1],[-1],[1],[-1]])\n",
    "assert np.abs(ErrorRate(wtest1,xtest1,ytest1)-0.25)<0.01 \n",
    "wtest2 = np.array([[-0.3],[0.4],[-3.0]])\n",
    "xtest2 = np.array([[0.1,-2.0,3.1],[9.3,-1.3,-0.4],[3.4,4.5,-1.6],[0.2,-2.4,-3.1]])\n",
    "ytest2 = np.array([[-1],[-1],[1],[1]])\n",
    "assert np.abs(ErrorRate(wtest2,xtest2,ytest2)-0.0)<0.01 \n",
    "wtest3 = np.array([[1.2],[-0.4],[3.0]])\n",
    "xtest3 = np.array([[0.1,-2.0,3.1],[9.3,-1.3,-0.4],[3.4,4.5,-1.6],[0.2,-2.4,-3.1],[8.1,-2.0,-1.0]])\n",
    "ytest3 = np.array([[1],[-1],[1],[1],[1]])\n",
    "assert np.abs(ErrorRate(wtest3,xtest3,ytest3)-0.6)<0.01 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d94d361aa917c1620c5c91ff6796e576",
     "grade": false,
     "grade_id": "Perceptron_Learning_Algorithm",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Perceptron Learning Algorithm [10 points]\n",
    "\n",
    "Implement a function for the perceptron learning algorithm (PLA) which accepts four arguments: an inital weight vector, the data, the labels (binary $\\{-1,+1\\}$), and the maximal number of iterations it executes. It is thus a version of the PLA that is assured to terminate. The function returns the updated weight vector. \n",
    "\n",
    "**Note:** To pass the test cases, your PLA needs to select the misclassified sample to perform a model update in a deterministic manner as follows: from the training data $\\{(x_1,y_1,), (x_2,y_2), \\ldots,(x_N,y_N)\\}$ select the misclassified sample $(x_i,y_i)$ with the smallest index $i$ for the PLA update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7811e25feb93926aecf91c64ff531b8a",
     "grade": false,
     "grade_id": "PLA",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def PLA(w, x, y, maxIter):\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    wIter = w\n",
    "    iteration = 0\n",
    "    while (iteration<maxIter) and (ErrorRate(wIter, x, y)>0):\n",
    "        # extract the misclassified sample with least index\n",
    "        y_hat = np.matmul(np.transpose(wIter), np.transpose(x)) # 1xN\n",
    "        y_hat[y_hat<0.0] = -1\n",
    "        y_hat[y_hat>0.0] = 1\n",
    "        errorArray = np.array(y_hat != np.transpose(y)) # 1xN\n",
    "        misIdx = 0\n",
    "        for idx in range(0, y_hat.shape[1]):\n",
    "            if errorArray[0, idx]:\n",
    "                misIdx = idx\n",
    "                break\n",
    "        xMis = np.reshape(x[misIdx, :], (1, x.shape[1])) # 1x(d+1)\n",
    "        # update weight\n",
    "        wIter = wIter + np.transpose(y[misIdx]*xMis) # (d+1)x1 + (d+1)x1\n",
    "        # increment iteration\n",
    "        iteration = iteration + 1\n",
    "    return wIter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3f2c13bea7a787a9a0f29011a4928dc3",
     "grade": true,
     "grade_id": "correct-PLA",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Check that PLA returns the correct output for several inputs\"\"\"\n",
    "\"\"\"(You may have to adjust the array format for inputs and output that your function expects and delivers, respectively.)\"\"\"\n",
    "wtest1 = np.array([[1],[1],[1]])\n",
    "xtest1 = np.array([[1,2,3],[2,3,4],[4,5,6],[-1,-2,-3]])\n",
    "ytest1 = np.array([[1],[-1],[1],[-1]])\n",
    "wtest2 = np.array([[-0.3],[0.4],[-3.0]])\n",
    "xtest2 = np.array([[0.1,-2.0,3.1],[9.3,-1.3,-0.4],[3.4,4.5,-1.6],[0.2,-2.4,-3.1]])\n",
    "ytest2 = np.array([[-1],[-1],[1],[1]])\n",
    "wtest3 = np.array([[1.2],[-0.4],[3.0]])\n",
    "xtest3 = np.array([[0.1,-2.0,3.1],[9.3,-1.3,-0.4],[3.4,4.5,-1.6],[0.2,-2.4,-3.1],[8.1,-2.0,-1.0]])\n",
    "ytest3 = np.array([[1],[-1],[1],[1],[1]])\n",
    "assert np.linalg.norm(np.subtract(PLA(wtest1,xtest1,ytest1, 100),np.array([[-4],[0],[4]])))<0.01\n",
    "assert np.linalg.norm(np.subtract(PLA(wtest1,xtest2,ytest2, 100),np.array([[-4.8],[6.4],[-6.4]])))<0.01\n",
    "assert np.linalg.norm(np.subtract(PLA(wtest3,xtest3,np.array([[1],[-1],[-1],[1],[1]]), 100), np.array([[-1.],[-4.2],[-1.3]])))<0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "15b86e1a140a12fe643595e9f8358cb0",
     "grade": false,
     "grade_id": "PLA_test",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now test the `PLA` function on the data set.\n",
    "\n",
    "Test the `PLA` function by training the classifier on the training data (`trainData`, `trainTarget`) with a maximum of 100 iterations, and measuring the cassification error using the testing data (`testData`, `testTarget`). Write the test script into the box below and let it print the classification error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a120b57a39d41f0d90c4cda31a54e1ca",
     "grade": true,
     "grade_id": "PLA_test_code",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification error is 0.041379310344827586\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "# load data\n",
    "trainData, validData, testData, trainTarget, validTarget, testTarget = loadData()\n",
    "# flatten x arrays and append the bias feature\n",
    "trainData = np.reshape(trainData, (trainData.shape[0], trainData.shape[1]*trainData.shape[2]))\n",
    "trainData = np.insert(trainData, 0, 1.0, axis=1)\n",
    "testData = np.reshape(testData, (testData.shape[0], testData.shape[1]*testData.shape[2]))\n",
    "testData = np.insert(testData, 0, 1.0, axis=1)\n",
    "# we also flatten and append the validation set here,\n",
    "# though not using it immediately\n",
    "validData = np.reshape(validData, (validData.shape[0], validData.shape[1]*validData.shape[2]))\n",
    "validData = np.insert(validData, 0, 1.0, axis=1)\n",
    "# scale targets for train, test, val set to {-1, +1}\n",
    "trainTarget = (trainTarget - 0.5)*2\n",
    "testTarget = (testTarget - 0.5)*2\n",
    "validTarget = (validTarget - 0.5)*2\n",
    "# train\n",
    "w0 = np.zeros((trainData.shape[1], 1))\n",
    "w = PLA(w0, trainData, trainTarget, 100)\n",
    "# test\n",
    "errorRate = ErrorRate(w, testData, testTarget)\n",
    "print(f\"Classification error is {errorRate}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8cfd4bb266d9689f4e551c9ae9e2a8ed",
     "grade": false,
     "grade_id": "Pocket",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Pocket algorithm [8 points]\n",
    "\n",
    "Implement a function for the pocket algorithm which accepts three arguments: the data, the labels (binary $\\{-1,+1\\}$), and the number of iterations it executes. It should use the function `PLA` you developed above. It returns the updated weight vector.\n",
    "\n",
    "**Note:** To pass the test cases, initialize the weight vector to the all-zero vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "35d855435c51392bc5aa2141a6847b92",
     "grade": false,
     "grade_id": "Pocket-algorithm",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def pocket(x, y, T):\n",
    " \n",
    "    # YOUR CODE HERE\n",
    "    w = np.zeros((x.shape[1], 1))\n",
    "    for t in range(0, T):\n",
    "        # run PLA once\n",
    "        wNew = PLA(w, x, y, 1)\n",
    "        # compute errors\n",
    "        error = ErrorRate(w, x, y)\n",
    "        errorNew = ErrorRate(wNew, x, y)\n",
    "        if errorNew < error:\n",
    "            w = wNew\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "267aeecabceba9389529bfa4ac0bd85f",
     "grade": true,
     "grade_id": "Pocket-correct",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Check that Pocket algorithm returns the correct output for several inputs\"\"\"\n",
    "\"\"\"(You may have to adjust the array format for inputs and output that your function expects and delivers, respectively.)\"\"\"\n",
    "xtest1 = np.array([[1,2,3],[2,3,4],[4,5,6],[-1,-2,-3]])\n",
    "ytest1 = np.array([[1],[-1],[1],[-1]])\n",
    "xtest2 = np.array([[0.1,-2.0,3.1],[9.3,-1.3,-0.4],[3.4,4.5,-1.6],[0.2,-2.4,-3.1]])\n",
    "ytest2 = np.array([[-1],[-1],[1],[1]])\n",
    "xtest3 = np.array([[0.1,-2.0,3.1],[9.3,-1.3,-0.4],[3.4,4.5,-1.6],[0.2,-2.4,-3.1],[8.1,-2.0,-1.0]])\n",
    "ytest3 = np.array([[1],[-1],[1],[1],[1]])\n",
    "assert np.linalg.norm(np.subtract(pocket(xtest1,ytest1, 10),np.array([[1.], [2.],[3.]])))<0.01\n",
    "assert np.linalg.norm(np.subtract(pocket(xtest2,ytest1, 5),np.array([[-9.2],[-0.7],[3.5]])))<0.01\n",
    "assert np.linalg.norm(np.subtract(pocket(xtest3,ytest3, 5),np.array([[ 0.1],[-2.],[3.1]])))<0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5062aecc4471bb9a43f0fab034336698",
     "grade": false,
     "grade_id": "Pocket_test",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Test the `pocket` function by training the classifier on the training data `(trainData, trainTarget)` with 100 iterations, and measuring the cassification error using the testing data `(testData, testTarget)`. Write the test script into the box below and let it print the classiciation error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ad49f5bc97b980ac9be8f19fc809b511",
     "grade": true,
     "grade_id": "Pocket_test_code",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification error is 0.08275862068965517\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "# train\n",
    "w = pocket(trainData, trainTarget, 100)\n",
    "# test\n",
    "errorRate = ErrorRate(w, testData, testTarget)\n",
    "print(f\"Classification error is {errorRate}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d10812918f61ea90aaf533e82c010294",
     "grade": false,
     "grade_id": "PLA_vs_Pocket",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "In the text box below, state the test error results for the PLA and pocket algorithm that you obtained. Briefly discuss if they are as you expected and why or why not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "08579e8a48ac13ae7992c0d6195774fa",
     "grade": true,
     "grade_id": "Discussion_PLA_vs_Pocket",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "The error from the PLA algorithm is 4.14%; the error from the pocket algorithm is 8.28%. So the pocket algorithm yields a larger error than the PLA algorithm.\n",
    "\n",
    "This is what I expected. the pocket algorithm yields a larger error due to the deterministic method our PLA algorithm picks misclassified samples. If during any iteration of the pocket algorithm, the PLA algorithm picks a sample that leads to higher loss, the same sample will be picked up again in the next iteration, and the pocket algorithm will no longer update the weights.\n",
    "\n",
    "If we employ a random picking strategy in our PLA algorithm, then the pocket algorithm may (or may not) be able to perform better than the PLA algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6fce9815992cbdbf3153f63725097338",
     "grade": false,
     "grade_id": "Logistic_Regression",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Logistic Regression\n",
    "\n",
    "**Note:** For classification with logistic regression, we consider that the **labels $y_n\\in\\{0,1\\}$** are used. So please write your responses and your functions accordingly.\n",
    "\n",
    "The second classifier we consider is logistic regression.  Logistic regression computes the probability measure\n",
    "$$\\hat{y}=\\theta(\\underline{w}^T\\underline{x})$$\n",
    "for a feature vector $\\underline{x}$, where $\\theta(z)=\\mathrm{e}^s/(1+\\mathrm{e}^s)$ is the logistic function, and $\\hat{y}$ is interpretated as $\\Pr(y=1)$. Given $N$ data samples $\\underline{x}_n$ and labels $y_n$, the error measure for logistic regression is the binary cross-entropy loss \n",
    "$$L(\\underline{w})=\\frac{1}{N}\\sum\\limits_{n=1}^N\\left[-y_n\\log\\left(\\hat{y}(\\underline{x}_n)\\right)-(1-y_n)\\log\\left(1-\\hat{y}(\\underline{x}_n)\\right)\\right]\\;.$$\n",
    "For the following, you will consider the regularized loss function\n",
    "$$L_\\lambda(\\underline{w})=\\frac{1}{N}\\sum\\limits_{n=1}^N\\left[-y_n\\log\\left(\\hat{y}(\\underline{x}_n)\\right)-(1-y_n)\\log\\left(1-\\hat{y}(\\underline{x}_n)\\right)\\right]+\\frac{\\lambda}{2}\\|\\underline{w}\\|_2^2$$\n",
    "with the regularization parameter $\\lambda \\ge 0$. \n",
    "\n",
    "The training of the weight vector $\\underline{w}$ will be performed through batch gradient descent. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a9b01e6026f02c97dee8cd141885a529",
     "grade": false,
     "grade_id": "Loss_function_lr",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Loss function [2 points]\n",
    "\n",
    "Implement a function to compute the regularized cross-entropy loss as defined above. The function has four input arguments: the weight vector, the feature vectors, the labels (binary $\\{0,1\\}$), and the regularization parameter. It returns the regularized loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "569a4bc56f7da0631a599b0c90cbe403",
     "grade": false,
     "grade_id": "CrossEntropyLoss",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return (1/(1+np.exp(-x)))\n",
    "\n",
    "def crossEntropyLoss(w, x, y, reg):\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    wx = np.matmul(np.transpose(w), np.transpose(x))\n",
    "    y_hat = np.transpose(sigmoid(wx)) # Nx1\n",
    "    loss = (np.sum(-1.0*y*np.log(y_hat)-(1.0-y)*np.log(1.0-y_hat)) / y.shape[0]) + reg/2.0 * np.sum(np.square(w))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a181689b41b6c6e955331205aed5258e",
     "grade": true,
     "grade_id": "CrossEntropyLoss-correct",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Check that CrossEntropyLoss returns the correct output for several inputs\"\"\"\n",
    "\"\"\"(You may have to adjust the array format for inputs and output that your function expects and delivers, respectively.)\"\"\"\n",
    "wtest1 = np.array([[1],[1],[1]])\n",
    "xtest1 = np.array([[1,2,3],[2,3,4],[4,5,6],[-1,-2,-3]])\n",
    "ytest1 = np.array([[1],[0],[1],[0]])\n",
    "wtest2 = np.array([[-0.3],[0.4],[-3.0]])\n",
    "xtest2 = np.array([[0.1,-2.0,3.1],[9.3,-1.3,-0.4],[3.4,4.5,-1.6],[0.2,-2.4,-3.1]])\n",
    "ytest2 = np.array([[0],[0],[1],[1]])\n",
    "wtest3 = np.array([[1.2],[-0.4],[3.0]])\n",
    "xtest3 = np.array([[0.1,-2.0,3.1],[9.3,-1.3,-0.4],[3.4,4.5,-1.6],[0.2,-2.4,-3.1],[8.1,-2.0,-1.0]])\n",
    "ytest3 = np.array([[1],[0],[1],[1],[1]])\n",
    "assert np.abs(crossEntropyLoss(wtest1,xtest1,ytest1,reg=0.0)-2.25)<0.01 \n",
    "assert np.abs(crossEntropyLoss(wtest2,xtest2,ytest2,reg=0.1)-0.49)<0.01 \n",
    "assert np.abs(crossEntropyLoss(wtest3,xtest3,ytest3,reg=0.1)-4.77)<0.01 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "69192297cd70b1359d0bdaca81a30b15",
     "grade": false,
     "grade_id": "Gradient",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Gradient [4 points]\n",
    "\n",
    "In the text box below, provide an analytical expression for the gradient of the regularized cross-entropy loss with respect to the weight vector. Note that the labels $y_n\\in\\{0,1\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f940cb03d538ba8d06659dc95003ea03",
     "grade": true,
     "grade_id": "Gradient_expression",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "We can take derivatives for the classification term and the regularization term separately.\n",
    "\n",
    "1. For the classification term,\n",
    "$$\\frac{\\partial L}{\\partial \\underline{w}} = \\frac{\\partial L}{\\partial \\hat{y}} \\frac{\\partial \\hat{y}}{\\partial \\underline{s}} \\frac{\\partial \\underline{s}}{\\partial \\underline{w}} = \\frac{1}{N}\\sum\\limits_{n=1}^N\\left[ (\\frac{\\hat{y}_n-y_n}{\\hat{y}_n(1-\\hat{y}_n)}) (\\hat{y}_n(1-y_n)) (\\underline{x}_n) \\right] = \\frac{1}{N}\\sum\\limits_{n=1}^N\\left[ (\\hat{y}_n-y_n) (\\underline{x}_n) \\right]$$, where $\\underline{s} = \\underline{w}^T\\underline{x}$. This can be rewritten as a multiplication of matrices:\n",
    "$$\\frac{\\partial L}{\\partial \\underline{w}} = \\frac{1}{N}(\\hat{\\underline{y}}-\\underline{y})^T X$$, where $X$ is the $N \\times (d+1)$ data matrix. \n",
    "2. For the regularization term,\n",
    "$$\\frac{\\partial L}{\\partial \\underline{w}} = \\lambda \\underline{w}$$.\n",
    "\n",
    "The gradient of the total loss therefore is\n",
    "$$\\frac{\\partial L}{\\partial \\underline{w}} = \\frac{1}{N}(\\hat{\\underline{y}}-\\underline{y})^T X + \\lambda \\underline{w}^T$$. The result is a $1 \\times (d+1)$ vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "54fc8b27e63b7fbda2e854b512b09380",
     "grade": false,
     "grade_id": "Gradient_implement",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Implement a function to compute the gradient. The function has four input arguments: the weight vector, the feature vectors, the labels (binary $\\{0,1\\}$), and see the test cases below for the array format) and the regularization parameter. It returns the gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0699ec9ddbd0084e5aafe64ad31be8ff",
     "grade": false,
     "grade_id": "Gradient_code",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def gradCE(w, x, y, reg):\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    # get N\n",
    "    N = x.shape[0]\n",
    "    # compute y_hat\n",
    "    wx = np.matmul(np.transpose(w), np.transpose(x))\n",
    "    y_hat = sigmoid(wx) # 1xN\n",
    "    # compute dL/dw\n",
    "    dLdw = (1.0/N * np.matmul((y_hat - np.transpose(y)), x)) + reg*np.transpose(w) # 1x(d+1)\n",
    "    dLdw = np.transpose(dLdw)\n",
    "    return dLdw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a8486cded118842f54dc5c9c3e09505c",
     "grade": true,
     "grade_id": "Gradient_code-correct",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Check that gradeCE returns the correct output for several inputs\"\"\"\n",
    "\"\"\"(You may have to adjust the array format for inputs and output that your function expects and delivers, respectively.)\"\"\"\n",
    "wtest1 = np.array([[1],[1],[1]])\n",
    "xtest1 = np.array([[1,2,3],[2,3,4],[4,5,6],[-1,-2,-3]])\n",
    "ytest1 = np.array([[1],[0],[1],[0]])\n",
    "wtest2 = np.array([[-0.3],[0.4],[-3.0]])\n",
    "xtest2 = np.array([[0.1,-2.0,3.1],[9.3,-1.3,-0.4],[3.4,4.5,-1.6],[0.2,-2.4,-3.1]])\n",
    "ytest2 = np.array([[0],[0],[1],[1]])\n",
    "wtest3 = np.array([[1.2],[-0.4],[3.0]])\n",
    "xtest3 = np.array([[0.1,-2.0,3.1],[9.3,-1.3,-0.4],[3.4,4.5,-1.6],[0.2,-2.4,-3.1],[8.1,-2.0,-1.0]])\n",
    "ytest3 = np.array([[1],[0],[1],[1],[1]])\n",
    "assert np.linalg.norm(np.subtract(gradCE(wtest1,xtest1,ytest1,reg=0),np.array([[0.50],[0.75],[1.0]])))<0.01\n",
    "assert np.linalg.norm(np.subtract(gradCE(wtest2,xtest2,ytest2,reg=1),np.array([[-0.05],[0.36],[-3.01]])))<0.01\n",
    "assert np.linalg.norm(np.subtract(gradCE(wtest3,xtest3,ytest3,reg=1),np.array([[2.39],[-1.01],[3.84]])))<0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "91511ba4db2e39b86f4d4e7da9e9ca5d",
     "grade": false,
     "grade_id": "Gradient_descent",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Gradient Descent Implementation [3 points]\n",
    "Using the gradient and cross-entropy loss function above, implement the batch gradient descent algorithm. The function should accept seven arguments: the weight vector, the feature vectors, the labels (binary $\\{0,1\\}$), the learning rate, the number of epochs, the regularization parameter, and an error tolerance (set to $10^{-7}$ for the experiments). The error tolerance will be used to terminate the gradient descent early, if the difference (i.e., its 2-norm) between the old and updated weights after one iteration is below the error tolerance. The function should return the optimized weight vector and the training loss. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1fa0d0f04a42710ba51901de84eb97b7",
     "grade": true,
     "grade_id": "Gradient_descent_code",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def grad_descent(w, x, y, eta, iterations, reg, error_tol):\n",
    "    # YOUR CODE HERE\n",
    "    # initialize weight and losses\n",
    "    wCurr = w\n",
    "    losses = []\n",
    "    # descend gradients\n",
    "    for t in range(0, iterations):\n",
    "        # compute gradient vector\n",
    "        g = gradCE(wCurr, x, y, reg)\n",
    "        # update weights\n",
    "        wNext = wCurr - eta * g\n",
    "        # compute loss\n",
    "        loss = crossEntropyLoss(wNext, x, y, reg)\n",
    "        losses.append(loss)\n",
    "        # terminate early if within error tolerence\n",
    "        if np.linalg.norm((wNext - wCurr), 'fro') < error_tol:\n",
    "            wCurr = wNext\n",
    "            break\n",
    "        else:\n",
    "            wCurr = wNext\n",
    "    # return last weights and losses\n",
    "    return (wCurr, losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a699411ecf32b1d1b9c08abc5b470116",
     "grade": false,
     "grade_id": "Tuning",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Tuning the Learning Rate [4 Points]: \n",
    "Write a script that excutes logistic regression using the gradient descent function from above to classify the two classes in the notMNIST dataset, loaded with the  `loadData` function. \n",
    "\n",
    "Set the number of epochs to $2,000$ and use the regularization parameter $\\lambda=0$. \n",
    "\n",
    "For the learning rate, consider $\\eta=5\\cdot 10^{-3},\\,10^{-3},\\,10^{-4}$.\n",
    "\n",
    "Train the classifier on the training data (`trainData, trainTarget`).\n",
    "\n",
    "The script should plot the training loss as a function of the number of training epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "50fce52cb17a97fb4b74037dade5e7f8",
     "grade": true,
     "grade_id": "Test_Gradient_Descent",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Loss')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "# set up hyperparameters\n",
    "iterations = 2000\n",
    "reg = 0\n",
    "error_tol = 1e-7\n",
    "\n",
    "# scale targets for train, test, val set to {0, 1}\n",
    "trainTarget = (trainTarget + 1)/2\n",
    "testTarget = (testTarget + 1)/2\n",
    "validTarget = (validTarget + 1)/2\n",
    "\n",
    "# train with learning rate 5e-3\n",
    "w0 = np.zeros((trainData.shape[1], 1))\n",
    "eta = 5e-3\n",
    "w, losses = grad_descent(w0, trainData, trainTarget, eta, iterations, reg, error_tol)\n",
    "ax.plot(losses, label='5e-3')\n",
    "\n",
    "# train with learning rate 1e-3\n",
    "w0 = np.zeros((trainData.shape[1], 1))\n",
    "eta = 1e-3\n",
    "w, losses = grad_descent(w0, trainData, trainTarget, eta, iterations, reg, error_tol)\n",
    "ax.plot(losses, label='1e-3')\n",
    "\n",
    "# train with learning rate 1e-4\n",
    "w0 = np.zeros((trainData.shape[1], 1))\n",
    "eta = 1e-4\n",
    "w, losses = grad_descent(w0, trainData, trainTarget, eta, iterations, reg, error_tol)\n",
    "ax.plot(losses, label='1e-4')\n",
    "\n",
    "# beautify the graph\n",
    "ax.legend(loc='best')\n",
    "ax.set_title('Training loss vs epoch curve at different learning rates')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2791726addc34f949dea41ae76193044",
     "grade": false,
     "grade_id": "Learning_rate",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "In the text box below, briefly discuss the impact of the learning rate $\\eta$ on the training time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f4e28efe96b79bed70627e2ea0eabeb9",
     "grade": true,
     "grade_id": "Learning_rate_discussion",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "From the graph above, we can see the training loss descends faster with larger learning rate. So larger learning rate allows us to reach the same loss in less iterations, thus saving training time.\n",
    "\n",
    "In practice however, a learning rate too large may lead to a gradient update \"skip over\" a minimum if the gradient is already at somewhere close to a minimum. In some cases, the loss may not converge. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d53d09ea57cafcea4cfac2f5055ab922",
     "grade": false,
     "grade_id": "Generalization",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Generalization [3 points]:\n",
    "Fix the learning rate to $\\eta=0.005$, and consider values for the regularization parameter $\\lambda = 0.001,\\, 0.01,\\, 0.1$. Measure the classification error using the validation and test sets and state them in your answer in the text box below. Comment on the effect of regularization on performance as well as the rationale behind tuning $\\lambda$ using the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f872837599eb156bbf8f47296f33e4f7",
     "grade": true,
     "grade_id": "Generalization_code",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lambda = 1e-3, Validation error rate is 0.05\n",
      "Lambda = 1e-3, Test error rate is 0.034482758620689655\n",
      "Lambda = 1e-2, Validation error rate is 0.05\n",
      "Lambda = 1e-2, Test error rate is 0.034482758620689655\n",
      "Lambda = 1e-1, Validation error rate is 0.05\n",
      "Lambda = 1e-1, Test error rate is 0.034482758620689655\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Here you can write your code for the generalization test\"\"\"\n",
    "\n",
    "# YOUR CODE HERE\n",
    "def Generalize(eta, lamb, iterations, error_tol, w0, evalData, evalTarget, dataStr):\n",
    "    w, losses = grad_descent(w0, trainData, trainTarget, eta, iterations, lamb, error_tol)\n",
    "    # we expect evalTarget to be in {0, 1}. Map target to {-1, +1}\n",
    "    evalTargetRemapped = evalTarget*2 - 1\n",
    "    errorRate = ErrorRate(w, evalData, evalTargetRemapped)\n",
    "    print(f\"{dataStr} error rate is {errorRate}\")\n",
    "    \n",
    "# set up common hyper-parameters\n",
    "eta = 5e-3\n",
    "iterations = 2000\n",
    "error_tol = 1e-7\n",
    "w0 = np.zeros((trainData.shape[1], 1))\n",
    "\n",
    "# train with lambda = .001\n",
    "reg1 = 1e-3\n",
    "Generalize(eta, reg1, iterations, error_tol, w0, validData, validTarget, 'Lambda = 1e-3, Validation')\n",
    "Generalize(eta, reg1, iterations, error_tol, w0, testData, testTarget, 'Lambda = 1e-3, Test')\n",
    "\n",
    "# train with lambda = .01\n",
    "reg2 = 1e-2\n",
    "Generalize(eta, reg2, iterations, error_tol, w0, validData, validTarget, 'Lambda = 1e-2, Validation')\n",
    "Generalize(eta, reg2, iterations, error_tol, w0, testData, testTarget, 'Lambda = 1e-2, Test')\n",
    "\n",
    "# train with lambda = .1\n",
    "reg3 = 1e-1\n",
    "Generalize(eta, reg3, iterations, error_tol, w0, validData, validTarget, 'Lambda = 1e-1, Validation')\n",
    "Generalize(eta, reg3, iterations, error_tol, w0, testData, testTarget, 'Lambda = 1e-1, Test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1bddd8b3fab681b640ae60bf60d797b4",
     "grade": true,
     "grade_id": "Generalization_results",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "The validation and test errors for the three lambda's, given 2000 training epochs are:\n",
    "1. For $\\lambda = .001$, val: 5%, test: 3.45%;\n",
    "2. For $\\lambda = .01$, val: 5%, test: 3.45%;\n",
    "3. For $\\lambda = .1$, val: 5%, test: 3.45%.\n",
    "\n",
    "Effects of regularization on performance:\n",
    "1. Regularization reduces overfitting by penalizing 1) large weights or 2) a weight vector for which the weights for a subset of features dominate.\n",
    "2. Regularization may lead to underfitting if $\\lambda$ is set to a value too large. For example, if we use 6000 iterations, then the test error rate with $\\lambda = 1$ becomes larger than smaller $\\lambda$'s.\n",
    "\n",
    "\n",
    "Rationale behind tuning $\\lambda$ with the validation set:\n",
    "* We tune $\\lambda$ with the validation set because overfitting happens when weights become too well-suited for the training set, but do not generalize well to other data. Since samples from the validation set are not from the training set, tuning with the validation set allows us to check the extent of overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
