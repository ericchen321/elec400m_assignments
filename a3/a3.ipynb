{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6f858b4535726fe4960f4587d7f5d944",
     "grade": false,
     "grade_id": "Intro",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Due date is Wednesday, April 7, 2021, 23:59 PST**\n",
    "\n",
    "**ONLY write into the existing cells, and do NOT delete or add any cells.**\n",
    "\n",
    "\n",
    "# ELEC 400M / EECE 571M Assignment 3: Clustering\n",
    "(Again, this assignment drew inspiration from an assignment used in ECE 421 at the University of Toronto and kindly made available to us by the instructor; as well as from the problem set in our textbook Learning from Data.)\n",
    "\n",
    "In this assignment, you will implement and test methods for clustering that we have discussed in the course.\n",
    "\n",
    "## Data Sets\n",
    "\n",
    "You will be working with two data sets:\n",
    "- data2D.npy\n",
    "- data100D.npy\n",
    "\n",
    "Each of the data sets has 10,000 data points, which are 2-dimensional and 100-dimensional, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3bb88c9f254c4347b16fcec408b9399e",
     "grade": false,
     "grade_id": "import",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "31dfb97406665284bcb9a389932c6b71",
     "grade": false,
     "grade_id": "K-Means",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## K-Means algorithm [18 marks]\n",
    "\n",
    "You will first implement and test the K-means algorithm. \n",
    "\n",
    "For this, you will **only use functions from the NumPy library**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b95497c5cfc6ade3a23a43f9258b559d",
     "grade": false,
     "grade_id": "Functions",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Implement the functions [5 marks]\n",
    "\n",
    "You will implement four functions to realize the K-Means algorithm.\n",
    "\n",
    "- `distanceFunc`: This function accepts your data and cluster centers and returns the squared pairwise distances between data points and cluster centers. \n",
    "- `KMinit`: This function accepts your data and returns the initial cluster centres. You will use the \"greedy approach\" described on page 6-16 in the textbook, but instead of a randomly selected data point always pick the first entry in the data set as the first center. \n",
    "- `lossFunc`: This function accepts the squared pairwise distances between data points and cluster centers and returns the error measure as defined in (6.5) in the textbook. \n",
    "- `KMmeans`: This function implements the K-Means algorithm as described in the textbook. It returns the cluster centers and the error measure as defined in (6.5) in the textbook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "22960d86b0cf61d695d43c40772b07f5",
     "grade": true,
     "grade_id": "distanceFunc",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def distanceFunc(x, mu):\n",
    "    # Inputs  \n",
    "    # x: is an NxD data matrix (N observations and D dimensions)\n",
    "    # mu: is an KxD cluster center matrix (K cluster centers and D dimensions)\n",
    "    # Output\n",
    "    # pair_dist2: is the NxK matrix of squared pairwise distances\n",
    "    \n",
    "    # YOUR CODE HERE \n",
    "    # initialize row and column indices for pair_dist2\n",
    "    row_nums = range(0, x.shape[0])\n",
    "    col_nums = range(0, mu.shape[0])\n",
    "    row_inds, col_inds = np.meshgrid(row_nums, col_nums, indexing='ij')\n",
    "    \n",
    "    # compute |x- mu|^2 with vectorization\n",
    "    xSubMu = x[row_inds] - mu[col_inds]\n",
    "    pair_dist = np.linalg.norm(xSubMu, axis=2)\n",
    "    pair_dist2 = np.square(pair_dist)\n",
    "    return pair_dist2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "adde9272e43237a9df615a449985f02c",
     "grade": true,
     "grade_id": "KMinit",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def computeDistToCluster(x, x_row, picked_rows):\n",
    "    # Inputs\n",
    "    # x: NxD data matrix\n",
    "    # x_row: row of a cluster center candidate\n",
    "    # picked_rows: list of rows picked as cluster centers\n",
    "    # Ouput\n",
    "    # distance from center candidate to picked centers\n",
    "    assert x_row not in picked_rows\n",
    "    \n",
    "    clusters = x[picked_rows, :] # (len(picked_rows), D)\n",
    "    candidate = x[x_row, :] # (D, )\n",
    "    \n",
    "    min_dist = -1\n",
    "    for row in range(0, clusters.shape[0]):\n",
    "        dist = np.linalg.norm(candidate - clusters[row, :])\n",
    "        if min_dist == -1 or dist < min_dist:\n",
    "            min_dist = dist\n",
    "    \n",
    "    return min_dist\n",
    "\n",
    "\n",
    "def KMinit(x, K):\n",
    "    # Inputs\n",
    "    # x: is an NxD data matrix \n",
    "    # K: number of clusters\n",
    "    # Output\n",
    "    # mu: is the KxD matrix of initial cluster centers using the \"greedy approach\" described on page 6-16 in the textbook. \n",
    "    # Remark: Always pick the first entry in the data set as the first center. \n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    # initialize mu\n",
    "    mu = np.zeros((K, x.shape[1]))\n",
    "    \n",
    "    # pick the first center\n",
    "    mu[0, :] = x[0, :]\n",
    "    picked_rows = [0] # list of rows in X picked as center\n",
    "    \n",
    "    # iteratively assign new cluster centers\n",
    "    assert K <= x.shape[0]\n",
    "    for mu_row in range(1, K):\n",
    "        row_selected = -1\n",
    "        max_dist = -1\n",
    "        # for each of the rest of points\n",
    "        for x_row in range(0, x.shape[0]):\n",
    "            if x_row not in picked_rows:\n",
    "                # compute distance to set of picked centers\n",
    "                dist = computeDistToCluster(x, x_row, picked_rows)\n",
    "                # pick the point with min distance as the new center\n",
    "                if dist > max_dist:\n",
    "                    row_selected = x_row\n",
    "                    max_dist = dist\n",
    "        mu[mu_row, :] = x[row_selected, :]\n",
    "        picked_rows.append(row_selected)\n",
    "    \n",
    "    return mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e3eb66b1f71b341e924a6308e4461720",
     "grade": true,
     "grade_id": "lossFunc",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def lossFunc(pair_dist2):\n",
    "    # Input \n",
    "    # pair_dist2: is an NxK matrix of squared pairwise distances\n",
    "    # Output\n",
    "    # loss: error as defined in (6.5) in the textbook\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    dist2_to_centers = np.amin(pair_dist2, axis=1)\n",
    "    loss = np.sum(dist2_to_centers)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0445ea8ccd19bf5e39c5adc28b22b6be",
     "grade": true,
     "grade_id": "Kmeans",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def computeCentroids(x, cluster_assigned):\n",
    "    # Inputs\n",
    "    # x: is an NxD data matrix\n",
    "    # cluster_assigned: (N, ) array of cluster index\n",
    "    # Outputs\n",
    "    # mu: is the KxD of cluster centers\n",
    "    K = np.amax(cluster_assigned) + 1\n",
    "    mu = np.zeros((K, x.shape[1]))\n",
    "    for cluster_index in range(0, K):\n",
    "        x_in_cluster = x[cluster_assigned==cluster_index, :]\n",
    "        mu[cluster_index, :] = np.mean(x_in_cluster, axis=0)\n",
    "    return mu\n",
    "\n",
    "def Kmeans(x,K):\n",
    "    # Inputs\n",
    "    # x: is an NxD data matrix \n",
    "    # K: number of clusters\n",
    "    # Outputs\n",
    "    # mu: is the KxD of cluster centers  \n",
    "    # loss: error as defined in (6.5) in the textbook \n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    # initialize mu and loss\n",
    "    mu = KMinit(x, K)\n",
    "    loss_last = -1.0\n",
    "    loss_curr = lossFunc(distanceFunc(x, mu))\n",
    "    \n",
    "    # until loss stops decreasing\n",
    "    while loss_last == -1.0 or abs(loss_curr - loss_last)>1e-9:\n",
    "        # update last loss\n",
    "        if loss_curr != -1.0:\n",
    "            loss_last = loss_curr\n",
    "        # cluster on mu\n",
    "        pair_dist2_before = distanceFunc(x, mu)\n",
    "        cluster_assigned = np.argmin(pair_dist2_before, axis=1) # (N, )\n",
    "        # update mu\n",
    "        mu = computeCentroids(x, cluster_assigned)\n",
    "        assert mu.shape[0] == K\n",
    "        # compute loss\n",
    "        pair_dist2_after = distanceFunc(x, mu)\n",
    "        loss_curr = lossFunc(pair_dist2_after)\n",
    "    \n",
    "    return mu, loss_curr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "31b1be0b4e175f346652755d66d26567",
     "grade": false,
     "grade_id": "test_functions",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Test the functions [5 marks]\n",
    "\n",
    "Develop toy-sized test cases (create your own very small data set(s)) for all four functions and verify the correct operation. \n",
    "\n",
    "Use the code cell below to program the test cases and to print and/or plot the results as appropriate. \n",
    "\n",
    "Use the text cell below to describe your test cases, to show the results and plots from your code, and to comment on the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b22140ba271f2433e9d9cd5639d873fd",
     "grade": true,
     "grade_id": "test_functions_code",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "def visualizeClusters(x, mu, title, legend_loc, filename, cluster_assigned=None, centroid_coloring='diff', plot_lines=True):\n",
    "    # Visualize clusters. Support at most 6 clusters.\n",
    "    # Implementation derived from visualization example from\n",
    "    # https://towardsdatascience.com/visualizing-clusters-with-pythons-matplolib-35ae03d87489\n",
    "    # Inputs\n",
    "    # x: Nx2 data matrix\n",
    "    # mu: Kx2 cluster centers\n",
    "    # title: plot title\n",
    "    # legend_loc: location of legend in figure\n",
    "    # filename: path to save figure\n",
    "    # cluster_assigned: (N, ) array of cluster assignments, optional\n",
    "    # centroid_coloring: 'same' or 'diff', optional\n",
    "    # plot_lines: if plot lines to connect point to centroid or not, optional\n",
    "    \n",
    "    # plot x and mu\n",
    "    fig = plt.figure(figsize=(8,8))\n",
    "    if cluster_assigned is None:\n",
    "        cluster_assigned = np.argmin(distanceFunc(x, mu), axis=1) # (N, )\n",
    "    K = mu.shape[0]\n",
    "    assert K <= 6\n",
    "    colors = ['blue', 'forestgreen', 'red', 'deepskyblue', 'sandybrown', 'turquoise']\n",
    "    for cluster_index in range(0, K):\n",
    "        x_in_cluster = x[cluster_assigned==cluster_index, :]\n",
    "        plt.scatter(x_in_cluster[:, 0], x_in_cluster[:, 1], c=colors[cluster_index], alpha=0.6, s=10, label=f\"Cluster {cluster_index}\")\n",
    "        centroid_color = colors[cluster_index]\n",
    "        if centroid_coloring == 'same':\n",
    "            centroid_color = 'black'\n",
    "        plt.scatter(mu[cluster_index, 0], mu[cluster_index, 1], marker='*', c=centroid_color, s=70, label=f\"Cluster centroid {cluster_index}\")\n",
    "    \n",
    "    # plot lines\n",
    "    if plot_lines:\n",
    "        for cluster_index in range(0, K):\n",
    "            x_in_cluster = x[cluster_assigned==cluster_index, :]\n",
    "            for row in range(0, x_in_cluster.shape[0]):\n",
    "                x1 = [x_in_cluster[row, 0], mu[cluster_index, 0]]\n",
    "                x2 = [x_in_cluster[row, 1], mu[cluster_index, 1]]\n",
    "                plt.plot(x1, x2, c=colors[cluster_index], alpha=0.2)\n",
    "            \n",
    "    # add legends, axis labels, title\n",
    "    plt.legend(loc=legend_loc, ncol=2)\n",
    "    \n",
    "    # add title and labels\n",
    "    plt.title(f'{title}\\n')\n",
    "    plt.xlabel('x1')\n",
    "    plt.ylabel('x2')\n",
    "    \n",
    "    # save figure\n",
    "    plt.savefig(filename)\n",
    "    \n",
    "    plt.close(fig)\n",
    "\n",
    "# tests for distanceFunc\n",
    "x = np.array([[2.5, 3.1], [4.1, 7.6], [0.8, 1.7]])\n",
    "mu = np.array([[0.2, 1.5], [3.5, 6.0]])\n",
    "dist2_expected = np.array([[7.85, 9.41], [52.42, 2.92], [0.4, 25.78]])\n",
    "dist2 = distanceFunc(x, mu)\n",
    "assert dist2.shape == dist2_expected.shape\n",
    "assert np.linalg.norm(dist2 - dist2_expected) < 1e-2\n",
    "visualizeClusters(x=x, mu=mu, title='distanceFunc()', legend_loc='upper left', filename='test_distanceFunc.png')\n",
    "\n",
    "# tests for KMInit\n",
    "X = np.array([[1.4, 2.3], [1.5, 2.0], [2.5, -1.0], [2.0, -0.5], [1.0, -1.0]])\n",
    "K = 3\n",
    "centers_expected = np.array([[1.4, 2.3], [2.5, -1.0], [1.0, -1.0]])\n",
    "centers = KMinit(X, K)\n",
    "assert np.linalg.norm(centers - centers_expected) < 1e-9\n",
    "visualizeClusters(x=X, mu=centers, title='KMInit()', legend_loc='upper right', filename='test_KMInit.png')\n",
    "\n",
    "# tests for lossFunc\n",
    "dist2_expected = np.array([[7.85, 9.41], [52.42, 2.92], [0.4, 25.78]])\n",
    "loss_expected = 7.85 + 2.92 + 0.4\n",
    "loss = lossFunc(dist2_expected)\n",
    "assert np.linalg.norm(loss - loss_expected) < 1e-9\n",
    "\n",
    "# tests for Kmeans\n",
    "# test case: 8 points, 2 clusters\n",
    "x = np.array([[-3, 2], [-2, 1], [2, -1], [3, -1], [-2, 2], [2, -2], [-3, 1], [3, -2]], dtype=np.float64)\n",
    "K = 2\n",
    "mu, loss = Kmeans(x, K)\n",
    "mu_expected = np.array([[-2.5, 1.5], [2.5, -1.5]])\n",
    "loss_expected = 4.0\n",
    "assert np.linalg.norm(mu - mu_expected)<1e-9 and np.linalg.norm(loss - loss_expected)<1e-9\n",
    "visualizeClusters(x=x, mu=mu, title='Kmeans, 8 points', legend_loc='upper right', filename='test_Kmeans_8pts.png')\n",
    "# test case: 1 point, 1 cluster\n",
    "x = np.array([[0.5, 0.5]])\n",
    "K = 1\n",
    "mu, loss = Kmeans(x, K)\n",
    "mu_expected = np.array([[0.5, 0.5]])\n",
    "loss_expected = 0.0\n",
    "assert np.linalg.norm(mu - mu_expected)<1e-9 and np.linalg.norm(loss - loss_expected)<1e-9\n",
    "visualizeClusters(x=x, mu=mu, title='Kmeans, 1 point', legend_loc='upper right', filename='test_Kmeans_1pt.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "841551b3a215872b8c3fc21d4da41942",
     "grade": true,
     "grade_id": "discuss_functions",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "1. **Tests for `distanceFunc()`.** Here we construct a test case where $x$ as a $3 \\times 2$ data matrix:\n",
    "$$x = \\begin{bmatrix}\n",
    "2.5 & 3.1\\\\ \n",
    "4.1 & 7.6\\\\ \n",
    "0.8 & 1.7\n",
    "\\end{bmatrix},$$\n",
    "and $\\mu$ as a $2 \\times 2$ matrix denoting two clusters:\n",
    "$$\\mu = \\begin{bmatrix}\n",
    "0.2 & 1.5\\\\ \n",
    "3.2 & 6.0\n",
    "\\end{bmatrix}.$$\n",
    "The pairwise squared distance matrix expected is a $3 \\times 2$ matrix:\n",
    "$$dist^{2}_{expected} = \\begin{bmatrix}\n",
    "7.85 & 9.41\\\\ \n",
    "52.42 & 2.92\\\\ \n",
    "0.4 & 25.78\n",
    "\\end{bmatrix}.$$\n",
    "The output from `distanceFunc()` is indeed the expected matrix. The diagram visualizes the points and the cluster centroids: ![](test_distanceFunc.png)\n",
    "\n",
    "2. **Tests for `KMInit()`.** Here we construct a test case with 5 2D points that are obvious in terms of how they should be clustered:\n",
    "$$x = \\begin{bmatrix}\n",
    "1.4 & 2.3\\\\ \n",
    "2.5 & 2.0\\\\ \n",
    "2.5 & -1.0\\\\\n",
    "2.0 & -0.5\\\\\n",
    "1.0 & -1.0\n",
    "\\end{bmatrix},$$ and we set $K=3$ for 3 clusters. The expected cluster centroids from `KMInit()` are $(1.4, 2.3)$, $(2.5, -1.0)$ and $(1.0, -1.0)$. From the assertion statement and the visualized clusters we can see our implementation can produce the correct result: ![](test_KMInit.png)\n",
    "\n",
    "3. **Tests for `lossFunc()`.** Here we construct a test case for which we reuse the pairwise squared distance matrix $dist^{2}_{expected}$ from the test for `distanceFunc()`. We know for each sample (row) its centroid is the one which yields the least squared distance. Thus the error as defined in (6.5) in the textbook is the sum of minimum distances along each row - $7.85+2.92+0.4$. The output from `lossFunc()` is as we expected.\n",
    "\n",
    "4. **Tests for `Kmeans()`.** Here we construct two test cases: 1) 8 samples that can be easily partitioned into two clusters:\n",
    "$$x = \\begin{bmatrix}\n",
    "-3 & 2\\\\ \n",
    "-2 & 1\\\\ \n",
    "2 & -1\\\\\n",
    "3 & -1\\\\\n",
    "-2 & 2\\\\\n",
    "2 & -2\\\\\n",
    "-3 & 1\\\\\n",
    "3 & -2\\\\\n",
    "\\end{bmatrix}.$$ The expected loss is $4$, and the assertion statement tells us `Kmeans()` can produce the correct loss. Also, from the visualization we can see Kmeans() can produce the correct centroids and clustering: ![](test_Kmeans_8pts.png)\n",
    "2) One sample. The loss should be trivially $0$ and the centroid should be itself. The assertion tells us `Kmeans()` got the loss right. Also we can see from the visualization the centroid and clustering are correct: ![](test_Kmeans_1pt.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d91321b08e6b361d97d2d69af2af38d2",
     "grade": false,
     "grade_id": "run_KMeans",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Run K-Means on 2D data set [4 marks]\n",
    "\n",
    "Implement a script that runs the K-Means algorithm using the functions above and run it for the data set data2D.npy with $K = 1, 2, 3, 4, 5$.  \n",
    "\n",
    "The script should produce the following plots:\n",
    "- for each of these values of $K$, a 2D scatter plot of the data points colored by their cluster assignments plus the cluster centers\n",
    "- the K-means loss as a function of $K$\n",
    "\n",
    "Show the plots in the text cell below and discuss how many clusters you think are \"best\" and why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0f06913cc097644f3f27ae7ded828342",
     "grade": true,
     "grade_id": "run_KMeans_code",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster at K=1\n",
      "Cluster at K=2\n",
      "Cluster at K=3\n",
      "Cluster at K=4\n",
      "Cluster at K=5\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "x = np.load('data2D.npy')\n",
    "losses_data2D = []\n",
    "\n",
    "# plot clusters\n",
    "for K in range(1, 6):\n",
    "    print(f'Cluster at K={K}')\n",
    "    mu, loss = Kmeans(x, K)\n",
    "    visualizeClusters(x=x, mu=mu, title=f'K={K}, loss={loss:.2f}', legend_loc='upper right', filename=(f'data2D_K={K}.png'), centroid_coloring='same', plot_lines=False)\n",
    "    losses_data2D.append(loss)\n",
    "\n",
    "# plot loss vs. K\n",
    "fig = plt.figure()\n",
    "plt.plot(range(1, 6), losses_data2D)\n",
    "plt.title('Loss vs. K\\n')\n",
    "plt.xlabel('K')\n",
    "plt.ylabel('loss')\n",
    "plt.savefig('data2D_loss_vs_K.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "703115d28706cac8b77a3098e05aec84",
     "grade": true,
     "grade_id": "Kmeans_discussion",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "Here we present the visualized clustering results:\n",
    "1. **The centroid is marked in black. You can find it by looking closely at the center of the image.**\n",
    "![](data2D_K=1.png)\n",
    "2. **The centroids are marked in black. You can find them by looking closely.**\n",
    "![](data2D_K=2.png)\n",
    "3. **The centroids are marked in black. You can find them by looking closely.**\n",
    "![](data2D_K=3.png)\n",
    "4. **The centroids are marked in black. You can find them by looking closely.**\n",
    "![](data2D_K=4.png)\n",
    "5. **The centroids are marked in black. You can find them by looking closely.**\n",
    "![](data2D_K=5.png)\n",
    "\n",
    "Here we present the loss vs. K graph:\n",
    "6. ![](data2D_loss_vs_K.png)\n",
    "7. **$K=3$ is the best.** We select $K$ using the elbow method: looking at the loss vs. K graph, we see loss drops rapidly from $K=1$ to $K=2$, but not as much from $K=2$ to $K=3$. So $3$ is the elbow of the graph. Also by observing the scatter plots of our data, we can identify two small blobs (one at lower left, one at upper right) and one large blob in the middle. So we consider $3$ being the best value for $K$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0e45e3b9884a374f0bd9197e3ee8e294",
     "grade": false,
     "grade_id": "Kmeans_gap",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Explore Gap statistic [4 marks]\n",
    "\n",
    "Implement a script which does the following using the functions implemented above.\n",
    "\n",
    "Generate benchmark random data of the same size as the 2D data set, distributed uniformly over the smallest axis-aligned rectangle containing the actual data. Run the K-Means algorithm on this random data for $K=1,2,3,4,5$. \n",
    "\n",
    "Repeat for 10 such random data sets to obtain the average of the log-K-Means errors, $$\\mathrm{LE}_{\\mathrm{in}}^{\\mathrm{rand}}(K)=\\frac{1}{10}\\sum\\limits_{i=1}^{10}\\log\\left(E_{\\mathrm{in},i}^{\\mathrm{rand}}(K)\\right)$$ as a function of $K$. \n",
    "\n",
    "Plot the average K-Means error and the gap statistic \n",
    "$$G(K)=\\mathrm{LE}_{\\mathrm{in}}^{\\mathrm{rand}}(K)-\\log \\left[E_{\\mathrm{in}}(K)\\right]$$\n",
    "as functions of $K$, where $E_{\\mathrm{in}}(K)$ is the K-Means error for the data set data2D.npy obtained above.\n",
    "\n",
    "Also plot the difference $G(K)-G(K+1)-s(K+1)$ as a function of $K$, \n",
    "where\n",
    "$$s(K)=\\sqrt{1+1/10} \\left[\\frac{1}{10} \\sum_{i=1}^{10} \\left(\\log \\left(E_{\\mathrm{in},i}^{\\mathrm{rand}}(K)\\right) - \\mathrm{LE}^{\\mathrm{rand}}_{\\mathrm{in}}(K)\\right)^2\\right]^{1/2}$$ \n",
    "is the scaled empirical standard deviation of the log-in-sample error.\n",
    "\n",
    "In the text cell below, show your plots and briefly explain the use of the gap statistic to determine the best number of clusters. Compare the result with your result from above. (Consult https://statweb.stanford.edu/~gwalther/gap for the necessary background.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c3e6e6abff0f0d4b648e01575948b5b2",
     "grade": true,
     "grade_id": "Kmeans_gap_code",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster at K=1 on benchmark set 0\n",
      "Cluster at K=1 on benchmark set 1\n",
      "Cluster at K=1 on benchmark set 2\n",
      "Cluster at K=1 on benchmark set 3\n",
      "Cluster at K=1 on benchmark set 4\n",
      "Cluster at K=1 on benchmark set 5\n",
      "Cluster at K=1 on benchmark set 6\n",
      "Cluster at K=1 on benchmark set 7\n",
      "Cluster at K=1 on benchmark set 8\n",
      "Cluster at K=1 on benchmark set 9\n",
      "Cluster at K=2 on benchmark set 0\n",
      "Cluster at K=2 on benchmark set 1\n",
      "Cluster at K=2 on benchmark set 2\n",
      "Cluster at K=2 on benchmark set 3\n",
      "Cluster at K=2 on benchmark set 4\n",
      "Cluster at K=2 on benchmark set 5\n",
      "Cluster at K=2 on benchmark set 6\n",
      "Cluster at K=2 on benchmark set 7\n",
      "Cluster at K=2 on benchmark set 8\n",
      "Cluster at K=2 on benchmark set 9\n",
      "Cluster at K=3 on benchmark set 0\n",
      "Cluster at K=3 on benchmark set 1\n",
      "Cluster at K=3 on benchmark set 2\n",
      "Cluster at K=3 on benchmark set 3\n",
      "Cluster at K=3 on benchmark set 4\n",
      "Cluster at K=3 on benchmark set 5\n",
      "Cluster at K=3 on benchmark set 6\n",
      "Cluster at K=3 on benchmark set 7\n",
      "Cluster at K=3 on benchmark set 8\n",
      "Cluster at K=3 on benchmark set 9\n",
      "Cluster at K=4 on benchmark set 0\n",
      "Cluster at K=4 on benchmark set 1\n",
      "Cluster at K=4 on benchmark set 2\n",
      "Cluster at K=4 on benchmark set 3\n",
      "Cluster at K=4 on benchmark set 4\n",
      "Cluster at K=4 on benchmark set 5\n",
      "Cluster at K=4 on benchmark set 6\n",
      "Cluster at K=4 on benchmark set 7\n",
      "Cluster at K=4 on benchmark set 8\n",
      "Cluster at K=4 on benchmark set 9\n",
      "Cluster at K=5 on benchmark set 0\n",
      "Cluster at K=5 on benchmark set 1\n",
      "Cluster at K=5 on benchmark set 2\n",
      "Cluster at K=5 on benchmark set 3\n",
      "Cluster at K=5 on benchmark set 4\n",
      "Cluster at K=5 on benchmark set 5\n",
      "Cluster at K=5 on benchmark set 6\n",
      "Cluster at K=5 on benchmark set 7\n",
      "Cluster at K=5 on benchmark set 8\n",
      "Cluster at K=5 on benchmark set 9\n",
      "s:\n",
      "[0.00550872 0.00776253 0.00650675 0.00673936 0.00943656]\n"
     ]
    }
   ],
   "source": [
    "# Retain this initialization of the random generato\n",
    "np.random.seed(421)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# generate 10 sets of benchmark data\n",
    "# acquire parameters for the smallest axis-aligned rectangle\n",
    "x1_min = np.amin(x[:, 0])\n",
    "x1_max = np.amax(x[:, 0])\n",
    "x2_min = np.amin(x[:, 1])\n",
    "x2_max = np.amax(x[:, 1])\n",
    "# generate 10 sets\n",
    "x_refs = []\n",
    "for ref_index in range(0, 10):\n",
    "    x_ref = np.zeros(x.shape)\n",
    "    x_ref[:, 0] = np.random.uniform(x1_min, x1_max+1e-9, x.shape[0])\n",
    "    x_ref[:, 1] = np.random.uniform(x2_min, x2_max+1e-9, x.shape[0])\n",
    "    x_refs.append(x_ref)\n",
    "    fig = plt.figure()\n",
    "    plt.plot(x_ref[:, 0], x_ref[:, 1])\n",
    "    plt.title(f'Benchmark set {ref_index}\\n')\n",
    "    plt.xlabel('x1')\n",
    "    plt.ylabel('x2')\n",
    "    plt.savefig(f'benchmark_set_{ref_index}.png')\n",
    "    plt.close(fig)\n",
    "\n",
    "# compute log error for each of the 10 sets, for K=1, 2, 3, 4, 5\n",
    "losses_x_ref = np.zeros((10, 5))\n",
    "logLosses_x_ref = np.zeros((10, 5))\n",
    "for K in range(1, 6):\n",
    "    for ref_index in range(0, 10):\n",
    "        print(f'Cluster at K={K} on benchmark set {ref_index}')\n",
    "        mu_ref, loss_ref = Kmeans(x_refs[ref_index], K)\n",
    "        visualizeClusters(x=x_refs[ref_index], mu=mu_ref, title=f'K={K}, loss={loss_ref:.2f}, reference set {ref_index}', legend_loc='upper right', filename=(f'data2D_ref_index={ref_index}_K={K}.png'), centroid_coloring='same', plot_lines=False)\n",
    "        losses_x_ref[ref_index, K-1] = loss_ref\n",
    "        logLosses_x_ref[ref_index, K-1] = np.log(loss_ref)\n",
    "# compute and plot mean log error on the 10 sets for K=1, 2, 3, 4, 5\n",
    "# first we plot mean error only\n",
    "meanLosses_x_ref = np.mean(losses_x_ref, axis=0) # (5, )\n",
    "fig = plt.figure()\n",
    "plt.plot(np.arange(1, 6), meanLosses_x_ref)\n",
    "plt.title('Mean error (NO LOG) vs. K\\n')\n",
    "plt.xlabel('K')\n",
    "plt.ylabel('Mean error (NO LOG)')\n",
    "plt.savefig(f'meanError_vs_K.png')\n",
    "plt.close(fig)\n",
    "# then we plot mean log error only\n",
    "meanLogLosses_x_ref = np.mean(logLosses_x_ref, axis=0) # (5, )\n",
    "fig = plt.figure()\n",
    "plt.plot(np.arange(1, 6), meanLogLosses_x_ref)\n",
    "plt.title('Mean log error vs. K\\n')\n",
    "plt.xlabel('K')\n",
    "plt.ylabel('Mean log error')\n",
    "plt.savefig(f'meanLogError_vs_K.png')\n",
    "plt.close(fig)\n",
    "# next we plot mean log error and log error on observed set vs. K side-by-side\n",
    "fig = plt.figure()\n",
    "plt.plot(np.arange(1, 6), meanLogLosses_x_ref, label='Mean log error')\n",
    "plt.plot(range(1, 6), np.log(np.array(losses_data2D)), label='Log error of 2D dataset')\n",
    "plt.legend(loc='upper right', ncol=1)\n",
    "plt.title('Mean log error and log error on 2D dataset vs. K\\n')\n",
    "plt.xlabel('K')\n",
    "plt.ylabel('log error')\n",
    "plt.savefig(f'meanLogError+logErrorOnData2D_vs_K.png')\n",
    "plt.close(fig)\n",
    "\n",
    "# compute and plot G(K)\n",
    "G = meanLogLosses_x_ref - np.log(np.array(losses_data2D)) # (5, )\n",
    "fig = plt.figure()\n",
    "plt.plot(np.arange(1, 6), G)\n",
    "plt.title('Gap statistic G(K) vs. K\\n')\n",
    "plt.xlabel('K')\n",
    "plt.ylabel('G')\n",
    "plt.savefig(f'G(K)_vs_K.png')\n",
    "plt.close(fig)\n",
    "\n",
    "# compute s(K)\n",
    "logDiff = logLosses_x_ref - np.tile(meanLogLosses_x_ref, (10, 1)) # (10, 5)\n",
    "sqrtMSE = np.sqrt(0.1 * np.sum(np.square(logDiff), axis=0)) # (5, )\n",
    "s = np.sqrt(np.array([1+0.1]))[0] * sqrtMSE # (5, )\n",
    "print('s:')\n",
    "print(s)\n",
    "\n",
    "# Compute and plot G(K) - G(K+1) + s(K+1)\n",
    "# We plot from K=1 to K=4\n",
    "criterion = np.zeros((4, ))\n",
    "for K in range(1, 5):\n",
    "    criterion[K-1] = G[K-1] - G[K] + s[K]\n",
    "G = meanLogLosses_x_ref - np.log(np.array(losses_data2D)) # (5, )\n",
    "fig = plt.figure()\n",
    "plt.plot(np.arange(1, 5), criterion)\n",
    "plt.title('G(K)-G(K+1)+s(K+1) vs. K\\n')\n",
    "plt.xlabel('K')\n",
    "plt.ylabel('G(K)-G(K+1)+s(K+1)')\n",
    "plt.savefig(f'G(K)-G(K+1)+s(K+1)_vs_K.png')\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9c2683dcda0972f5381dcd1afeb2118e",
     "grade": true,
     "grade_id": "Gap_discussion",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "1. **Plot of Mean error (No log + log) on the 10 benchmark sets for $K$ from 1 to 5:**\n",
    "Here we show mean error $\\mathrm{E}_{\\mathrm{in}}^{\\mathrm{rand}}(K)$ only:\n",
    "![](meanError_vs_K.png)\n",
    "Here we show mean log error $\\mathrm{LE}_{\\mathrm{in}}^{\\mathrm{rand}}(K)$ only:\n",
    "![](meanLogError_vs_K.png)\n",
    "Also, we plot $\\mathrm{LE}_{\\mathrm{in}}^{\\mathrm{rand}}(K)$ and the log error obtained from the actual observed 2D dataset side-by-side, for easier comparison against Figure 1(c) from https://statweb.stanford.edu/~gwalther/gap:\n",
    "![](meanLogError+logErrorOnData2D_vs_K.png)\n",
    "\n",
    "2. **Plot of Gap statistic $G(K)$ for $K$ from 1 to 5:**\n",
    "![](G(K)_vs_K.png)\n",
    "\n",
    "3. **Plot of the difference $G(K) - G(K+1) + s(K+1)$ for $K$ from 1 to 4:**\n",
    "![](G(K)-G(K+1)+s(K+1)_vs_K.png)\n",
    "\n",
    "4. **Explain the use of the gap statistic to determine the best number of clusters:**\n",
    "For this method, we choose the smallest $K$ which gives $G(K) - G(K+1) + s(K+1) \\geq 0$. The idea is to find the $K$ which yields the largest gap $G(K)$ between the mean-log-error of the benchmark sets and the log error of the actual observed set. The intuition behind finding the largest gap is, we build a null hypothesis that the observed set follows the same distribution as the benchmark sets, and the $K$ which yields the largest $G(K)$ is the best rejection to the hypothesis.\n",
    "\n",
    "5. **Compare result with the elbow method:** Following the gap method, we observe the minimum $K$ which gives $G(K) \\geq G(K+1) - s(K+1)$ is $3$. So we pick $K=3$ as the number of clusters for our observed 2D dataset, and the value agrees with our result from the elbow method. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2a5e3df1c7583553e470d619d8d0b8da",
     "grade": false,
     "grade_id": "GMM",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Soft-clustering with the Mixture of Gaussians [7 marks]\n",
    "\n",
    "Now you will implement and test soft-clustering using density estimation with Gaussian mixture models. For this you will use the functions from the **scikit-learn library**.\n",
    "\n",
    "First, write a script that, for the 2D data set data2D.npy,\n",
    "- estimates the parameters of the Gaussian mixture models with $K=3$ components,\n",
    "- plots the contours of the density,\n",
    "- plots the decision boundaries for clustering (in the same plot). \n",
    "\n",
    "In the text cell below, show the plot, interpret what you see in the plot and compare with the plot for $K=3$ clusters for the K-Means algorithm above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0e27d60acb88a327729371fc05c03821",
     "grade": true,
     "grade_id": "GMM_code",
     "locked": false,
     "points": 1.5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "# fit a Gaussian mixture model with K=3\n",
    "model_gm = GaussianMixture(n_components=3, random_state=0).fit(x)\n",
    "\n",
    "# plot contours\n",
    "# implementation derived from https://scikit-learn.org/stable/auto_examples/mixture/plot_gmm_pdf.html\n",
    "# construct a (50, 50) grid over the smallest axis-aligned rectangle\n",
    "x1 = np.linspace(x1_min, x1_max)\n",
    "x2 = np.linspace(x2_min, x2_max)\n",
    "x1_grid, x2_grid = np.meshgrid(x1, x2) # x1_grid and x2_grid: (50, 50)\n",
    "x1x2 = np.array([x1_grid.ravel(), x2_grid.ravel()]).T # (2500, 2)\n",
    "# compute log likelihood for every point on the grid\n",
    "logProb = model_gm.score_samples(x1x2) # (2500, )\n",
    "logProb = np.reshape(logProb, x1_grid.shape) # (50, 50)\n",
    "# plot contour\n",
    "fig = plt.figure(figsize=(12,8))\n",
    "contourSet = plt.contour(x1_grid, x2_grid, logProb)\n",
    "plt.colorbar(contourSet)\n",
    "plt.scatter(x[:, 0], x[:, 1], c='blue', alpha=0.6, s=1)\n",
    "\n",
    "# plot decision boundaries\n",
    "# implementation derived from https://scikit-learn.org/stable/auto_examples/ensemble/plot_voting_decision_regions.html\n",
    "# predict bump label for every point on the grid\n",
    "yhat = model_gm.predict(x1x2) # (2500, )\n",
    "yhat = np.reshape(yhat, x1_grid.shape)\n",
    "# plot color-filled contour\n",
    "plt.contourf(x1_grid, x2_grid, yhat, alpha=0.4)\n",
    "\n",
    "# add labels, etc and save figure\n",
    "plt.title('Log-likelihood and decision boundaries, Gaussian mixture model, K=3')\n",
    "plt.savefig('contours+decision_boundaries_K=3.png')\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2e6907f02197997522c4686beb072bdf",
     "grade": true,
     "grade_id": "GMM_discussion",
     "locked": false,
     "points": 1.5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "1. **Plot:** Here we have plotted log likelihood $\\log P(\\mathbf{x})$ and decision boundaries from predicting with a Gaussian mixture model with $3$ bumps:\n",
    "![](contours+decision_boundaries_K=3.png)\n",
    "\n",
    "2. **Compare with result from Kmeans:** For easy comparison, we put the Kmeans clustering result under $K=3$ here:\n",
    "![](data2D_K=3.png)\n",
    "1) We see soft clustering with Gaussian mixture model yields a better result - for points that clearly belong to the large blob at the center but are closer to the centers of the two small clusters, Kmeans is more prone to classify them as members of the smaller clusters. Soft clustering however can classify most of them correctly. 2) For points close to the three cluster centers, both algorithms can produce comparably good results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6929ca2dc9a168a5a4df49adb999dbcb",
     "grade": false,
     "grade_id": "GMM_BIC",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Write a script that trains the Gaussian mixture model for the data set data2D.npy for $K=1,2,3,4,5$, and computes and plots the Bayesian Information Criterion (BIC) as a function of $K$.\n",
    "\n",
    "In the text cell below, show the plot, explain the BIC and what model size it suggests, and compare it with your result for the K-Means algorithm above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "72674d040b273e893831893f0b18a38d",
     "grade": true,
     "grade_id": "GMM_BIC_code",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# compute BIC for K=1, 2, 3, 4, 5\n",
    "bics = []\n",
    "for K in range(1, 6):\n",
    "    # fit GMM\n",
    "    model_gm = GaussianMixture(n_components=K, random_state=0).fit(x)\n",
    "    # compute BIC\n",
    "    bic = model_gm.bic(x)\n",
    "    bics.append(bic)\n",
    "    \n",
    "# plot BIC vs. K\n",
    "fig = plt.figure()\n",
    "plt.plot(range(1, 6), bics)\n",
    "for K in range(1, 6):\n",
    "    plt.annotate(f'{bics[K-1]:.2f}', xy=(K, bics[K-1]))\n",
    "plt.title('BIC vs K')\n",
    "plt.xlabel('K')\n",
    "plt.ylabel('BIC')\n",
    "plt.savefig('bic_vs_K.png')\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "08b1b6fcaf429324583e4c782d083acc",
     "grade": true,
     "grade_id": "GMM_BIC_discussion",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "1. **Plot of BIC for $K$ from 1 to 5:**\n",
    "![](bic_vs_K.png)\n",
    "\n",
    "2. **Explain:** BIC is a criterion for evaluating how good a model fits a dataset. In the context of clustering, by evaluating models fit with different number of clusters, we can decide what the best number of clusters is. BIC is formulated as $$ -2\\log{P(D | \\hat{\\theta_{K}})} + m\\log{N}$$, where $D$ is a training set (as in our case, the 2D dataset), $m$ is the number of parameters in a model and $N$ is the number of samples in the training set. The lower BIC is, the better a model is. This makes sense since the formula has two terms: 1) the $\\log$ term which is smaller if likelihood $P(D | \\hat{\\theta_{K}})$ is higher; 2) a regularization term which penalizes larger models (with higher $m$) and larger training set, thus being smaller for less number of parameters which could be less prone to overfit. For our 2D dataset, we see the BIC value is the lowest at $K=3$. Therefore we pick $3$ as the number of clusters.\n",
    "\n",
    "3. **Compare with Kmeans:** For convenience, we put plots from the elbow and gap statistic method for Kmeans here:\n",
    "![](data2D_loss_vs_K.png)\n",
    "![](G(K)-G(K+1)+s(K+1)_vs_K.png)\n",
    "We see in all three cases we get $K=3$ being the best number of clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c66b3238c592d1ee3b74512b4fb079d0",
     "grade": false,
     "grade_id": "K_Means_GMM",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Implement a script that runs both the K-Means and the Gaussian mixture learning algorithms on data100D.npy for $K = \\{3,4,5,6\\}$ and plots the K-Means loss and the BIC as a function of $K$, respectively. \n",
    "\n",
    "Include the plots in the text cell below, and discuss how many clusters you think are within the dataset and compare the learnt results of K-means and Mixture of Gaussian approach.\n",
    "\n",
    "Remark: The Gaussian mixture learning may take some time to complete (5-10 mins).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6385e919a93b26bac21a064e32d68590",
     "grade": true,
     "grade_id": "K_Means_GMM_code",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering with Kmeans at K=3\n",
      "Clustering with GMM at K=3\n",
      "Clustering with Kmeans at K=4\n",
      "Clustering with GMM at K=4\n",
      "Clustering with Kmeans at K=5\n",
      "Clustering with GMM at K=5\n",
      "Clustering with Kmeans at K=6\n",
      "Clustering with GMM at K=6\n",
      "loss by Kmeans model:  215260.45\n",
      "loss by GMM:  215260.45\n",
      "Compare centroids:\n",
      "Here we get a sense of the scale of each dimension by computing means along each feature dimension\n",
      "[ 0.02065071 -0.02937015 -0.15677053  0.14341743  0.20807746 -0.16755158\n",
      " -0.0199376  -0.11346172 -0.1057238  -0.19177012  0.02990286 -0.19059206\n",
      " -0.06626135 -0.08542117 -0.08110166  0.16197014 -0.10936639  0.15577614\n",
      "  0.05326725  0.06372831  0.17548401 -0.12602571 -0.01677712 -0.05779135\n",
      "  0.25884825 -0.06590489 -0.08247374  0.15395891  0.12239977  0.30209236\n",
      " -0.17698428  0.02512007  0.1216058   0.25125387  0.19395779  0.06642044\n",
      " -0.11115155 -0.15146562 -0.01208015 -0.10174278  0.05076113  0.17101526\n",
      "  0.11554699 -0.05749478  0.06228279 -0.25435176  0.23398333  0.15514256\n",
      "  0.2279572  -0.05488515  0.23227266  0.27461787  0.03238829 -0.12522545\n",
      " -0.04128901 -0.03150513  0.12568308 -0.03091252  0.01560887 -0.13570793\n",
      " -0.07126126  0.07068848  0.00688766  0.12650397  0.09518241  0.14929686\n",
      " -0.03025174  0.28721774  0.06835039  0.26373149  0.03243356 -0.18385502\n",
      " -0.30919886  0.10276183 -0.1070662  -0.09838604  0.19636108 -0.01734671\n",
      " -0.14352651 -0.11301506 -0.17077975  0.18982209  0.22193233 -0.14172543\n",
      " -0.04281908  0.13977047  0.02498397 -0.12634529  0.0326605  -0.1729273\n",
      "  0.01627167  0.14588365 -0.11240383 -0.10008078  0.18599468  0.24941904\n",
      "  0.17461705 -0.19953627  0.16345094 -0.03293337]\n",
      "[ 0.02065085 -0.02937023 -0.15677058  0.14341698  0.20807771 -0.1675519\n",
      " -0.01993769 -0.11346242 -0.10572389 -0.19176978  0.0299025  -0.19059187\n",
      " -0.06626173 -0.08542123 -0.08110144  0.16196996 -0.10936642  0.15577642\n",
      "  0.05326741  0.06372841  0.17548375 -0.12602577 -0.01677701 -0.05779145\n",
      "  0.25884847 -0.06590508 -0.08247366  0.15395939  0.12239952  0.30209234\n",
      " -0.17698425  0.02512009  0.12160539  0.25125378  0.19395787  0.06642017\n",
      " -0.11115122 -0.15146533 -0.01208019 -0.10174277  0.05076109  0.17101515\n",
      "  0.11554671 -0.05749492  0.06228281 -0.25435182  0.23398327  0.15514241\n",
      "  0.22795763 -0.05488556  0.2322729   0.2746176   0.03238882 -0.1252255\n",
      " -0.04128885 -0.03150564  0.12568255 -0.03091275  0.01560887 -0.1357078\n",
      " -0.07126096  0.07068893  0.00688758  0.12650425  0.0951826   0.14929676\n",
      " -0.03025249  0.2872174   0.06835026  0.26373185  0.03243367 -0.18385518\n",
      " -0.3091986   0.10276237 -0.10706642 -0.09838614  0.19636092 -0.01734677\n",
      " -0.1435264  -0.11301504 -0.17077997  0.18982209  0.2219322  -0.14172537\n",
      " -0.04281881  0.13977042  0.02498433 -0.1263452   0.03265999 -0.17292749\n",
      "  0.01627167  0.14588338 -0.11240367 -0.10008056  0.18599442  0.24941889\n",
      "  0.17461724 -0.19953619  0.16345116 -0.03293345]\n",
      "Here we compute the norm of total difference between the centroids\n",
      "29.12538244818225\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "x100D = np.load('data100D.npy')\n",
    "\n",
    "losses_data100D = []\n",
    "bics_data100D = []\n",
    "# build Kmeans and GMM\n",
    "for K in range(3, 7):\n",
    "    # cluster with Kmeans and compute loss\n",
    "    print(f'Clustering with Kmeans at K={K}')\n",
    "    mu, loss = Kmeans(x100D, K)\n",
    "    losses_data100D.append(loss)\n",
    "    # cluster with GMM and compute BIC\n",
    "    print(f'Clustering with GMM at K={K}')\n",
    "    model_gm = GaussianMixture(n_components=K, random_state=0).fit(x100D)\n",
    "    # compute BIC\n",
    "    bic = model_gm.bic(x100D)\n",
    "    bics_data100D.append(bic)\n",
    "\n",
    "# plot loss vs. K\n",
    "fig = plt.figure()\n",
    "plt.plot(range(3, 7), losses_data100D)\n",
    "for K in range(3, 7):\n",
    "    plt.annotate(f'{losses_data100D[K-3]:.2f}', xy=(K, losses_data100D[K-3]))\n",
    "plt.title('Loss vs. K for 100D data, KMeans model\\n')\n",
    "plt.xlabel('K')\n",
    "plt.ylabel('loss')\n",
    "plt.savefig('data100D_loss_vs_K.png')\n",
    "plt.close(fig)\n",
    "\n",
    "# plot BIC vs. K\n",
    "fig = plt.figure()\n",
    "plt.plot(range(3, 7), bics_data100D)\n",
    "for K in range(3, 7):\n",
    "    plt.annotate(f'{bics_data100D[K-3]:.2f}', xy=(K, bics_data100D[K-3]))\n",
    "plt.title('BIC vs. K for 100D data, GMM\\n')\n",
    "plt.xlabel('K')\n",
    "plt.ylabel('BIC')\n",
    "plt.savefig('data100D_bic_vs_K.png')\n",
    "plt.close(fig)\n",
    "\n",
    "# At K=5, compare cluster centroids and losses from Kmeans and GMM\n",
    "K = 5\n",
    "# compute centroids and loss of Kmeans model\n",
    "centroids_kmeans, loss_kmeans = Kmeans(x100D, K)\n",
    "print(f'loss by Kmeans model: {loss_kmeans: .2f}')\n",
    "# compute centroids loss of GMM\n",
    "model_gmm = GaussianMixture(n_components=K, random_state=0).fit(x100D)\n",
    "centroids_gmm = model_gmm.means_\n",
    "loss_gmm = lossFunc(distanceFunc(x100D, centroids_gmm))\n",
    "print(f'loss by GMM: {loss_gmm: .2f}')\n",
    "# compare centroids\n",
    "print('Compare centroids:')\n",
    "print('Here we get a sense of the scale of each dimension by computing means along each feature dimension')\n",
    "print(np.mean(centroids_kmeans, axis=0))\n",
    "print(np.mean(centroids_gmm, axis=0))\n",
    "print('Here we compute the norm of total difference between the centroids')\n",
    "print(np.linalg.norm(centroids_gmm - centroids_kmeans))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1cae41bc1208ffebfaf828b386e52a3e",
     "grade": true,
     "grade_id": "K_Means_GMM_discussion",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "1. **Plots:**\n",
    "![](data100D_loss_vs_K.png)\n",
    "![](data100D_bic_vs_K.png)\n",
    "\n",
    "2. **How many clusters:** We choose $K=5$, because it is both the elbow of the loss graph and the minimum in the BIC graph.\n",
    "\n",
    "3. **Compare results:** Since visualizing clusters in a 100-D space is difficult, we use loss defined in 6.5 in the texbook as a metric to compare the quality of our two models. Also, we only compare Kmeans and GMM at $K=5$. We get $$E_{in}(\\textrm{Kmeans}, D) = E_{in}(\\textrm{GMM}, D) = 215260.45.$$ So both models yield the same loss. This result suggests that at 100 dimensions, in the case of our data, Kmeans and GMM can perform equally well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
